{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e90ffa1",
   "metadata": {},
   "source": [
    "This is notebook for output parsers in langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea23beb",
   "metadata": {},
   "source": [
    "Output Parsers in LangChain are classes that structure and validate the raw text output from LLMs into a more usable format. They ensure the model's response conforms to a specific schema or structure that your application expects.\n",
    "\n",
    "Why Use Output Parsers?\n",
    "Consistency: Get structured data instead of free-form text\n",
    "\n",
    "Validation: Catch malformed responses early\n",
    "\n",
    "Type safety: Convert strings to proper Python objects\n",
    "\n",
    "Reliability: Ensure downstream code receives expected formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670d565",
   "metadata": {},
   "source": [
    "1. StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf3ab8",
   "metadata": {},
   "source": [
    "String Output Parsers in LangChain are the simplest type of output parsers that handle basic text processing and transformation of LLM outputs. They work with raw string responses and provide various ways to clean, format, and structure text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c23fc419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai  import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80fc9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()\n",
    "model=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dc406a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model.invoke(\"Hello, how are you?\")\n",
    "parsed_response=parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9efa1e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f27f9",
   "metadata": {},
   "source": [
    "second example of stroutput parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "949220f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black hole is a region of spacetime with gravity so intense that nothing, not even light, can escape its event horizon. They typically form from the collapsed remnants of massive stars, with supermassive versions residing at the centers of most galaxies. Although invisible, they are detected by their gravitational effects on nearby stars, the bright accretion disks of matter swirling into them, and gravitational waves from their mergers. These objects are crucial to galaxy evolution and challenge our fundamental understanding of physics.\n"
     ]
    }
   ],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 line summary on the following text. /n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "prompt1 = template1.invoke({'topic':'black hole'})\n",
    "\n",
    "result = model.invoke(prompt1)\n",
    "\n",
    "prompt2 = template2.invoke({'text':result.content})\n",
    "\n",
    "result1 = model.invoke(prompt2)\n",
    "\n",
    "print(result1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "072ce71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bfb91f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parser.invoke(result1.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c9f31",
   "metadata": {},
   "source": [
    "2.jsonoutputparsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e54c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "163c8808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'topic': 'black hole', 'facts': [\"A black hole's gravitational pull is so strong that nothing, not even light, can escape once it crosses the event horizon.\", 'The center of a black hole is a point of infinite density called a singularity, where the laws of physics as we know them break down.', 'Supermassive black holes, millions or even billions of times the mass of our Sun, are thought to reside at the center of most large galaxies, including our own Milky Way.', 'Black holes are formed from the remnants of massive stars that collapse under their own gravity in a supernova explosion.', 'The first-ever direct image of a black hole was captured by the Event Horizon Telescope collaboration and released in 2019.']}\n",
      "Type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"\"\"Give me 5 facts about {topic} as a JSON array of strings.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \n",
    "    \n",
    "    Topic: {topic}\"\"\",\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instructions': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "result = chain.invoke({'topic': 'black hole'})\n",
    "\n",
    "print(\"Result:\", result)\n",
    "print(\"Type:\", type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225ed39",
   "metadata": {},
   "source": [
    "3.structure outpput parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c128829",
   "metadata": {},
   "source": [
    "4 pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "class Person(BaseModel):\n",
    "\n",
    "    name: str = Field(description='Name of the person')\n",
    "    age: int = Field(gt=18, description='Age of the person')\n",
    "    city: str = Field(description='Name of the city the person belongs to')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "final_result = chain.invoke({'place':'sri lankan'})\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45be996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b1498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain UV Environment",
   "language": "python",
   "name": "langchain-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
