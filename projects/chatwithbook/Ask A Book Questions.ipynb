{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b715375",
   "metadata": {},
   "source": [
    "# Top-K Similarity Search - Ask A Book A Question\n",
    "\n",
    "In this tutorial we will see a simple example of basic retrieval via Top-K Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d615a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain --upgrade\n",
    "# Version: 0.0.164\n",
    "\n",
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3e92ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "### Load your data\n",
    "\n",
    "Next let's load up some data. I've put a few 'loaders' on there which will load data from different locations. Feel free to use the one that suits you. The default one queries one of Paul Graham's essays for a simple example. This process will only stage the loader, not actually load it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d38044",
   "metadata": {},
   "source": [
    "Then let's go ahead and actually load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b93415",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Net Centric Computing chapter 1 solutions.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcdac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a744a",
   "metadata": {},
   "source": [
    "Then let's actually check out what's been loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4fd7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 160 document(s) in your data\n",
      "There are 0 characters in your sample document\n",
      "Here is a sample: \n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[0].page_content[:200]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "### Chunk your data up into smaller documents\n",
    "\n",
    "While we could pass the entire essay to a model w/ long context, we want to be picky about which information we share with our model. The better signal to noise ratio we have the more likely we are to get the right answer.\n",
    "\n",
    "The first thing we'll do is chunk up our document into smaller pieces. The goal will be to take only a few of those smaller pieces and pass them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879873a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 660 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many small chunks we have\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "### Create embeddings of your documents to get ready for semantic search\n",
    "\n",
    "Next up we need to prepare for similarity searches. The way we do this is through embedding our documents (getting a vector per document).\n",
    "\n",
    "This will help us compare documents later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373e695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_68036\\30168171.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e7857",
   "metadata": {},
   "source": [
    "Check to see if there is an environment variable with you API keys, if not, use what you put below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d66c06",
   "metadata": {},
   "source": [
    "### Option #1: Chroma (for local)\n",
    "\n",
    "I like Chroma becauase it's local and easy to set up without an account.\n",
    "\n",
    "First we'll pass our texts to Chroma via `.from_documents`, this will 1) embed the documents and get a vector, then 2) add them to the vectorstore for retrieval later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e0d1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(texts, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4750ab",
   "metadata": {},
   "source": [
    "Let's test it out. I want to see which documents are most closely related to a query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34929595",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What peter thiel thinks about startups?\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec60de1",
   "metadata": {},
   "source": [
    "Then we can check them out. In theory, the texts which are deemed most similar should hold the answer to our question.\n",
    "But keep in mind that our query just happens to be a question, it could be a random statement or sentence and it would still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0f5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national security and global finance. He has provided early funding for LinkedIn, Yelp, and dozens of\n",
      "successful technology startups, many run by former colleagues who have been dubbed the “PayPal\n",
      "Mafia.” He is a partner at Founders Fund, a Silicon Valley venture capital firm that has funded\n",
      "companies like SpaceX and Airbnb. He started the Thiel Fellowship, which ignited a national debate\n",
      "by encouraging young people to put learning before schooling, and he leads the Thiel Foundation,\n",
      "\n",
      "About the Authors\n",
      "Peter Thiel\n",
      " is an entrepreneur and investor. He started PayPal in 1998, led it as CEO, and took it public in\n",
      "2002, defining a new era of fast and secure online commerce. In 2004 he made the first outside\n",
      "investment in Facebook, where he serves as a director. The same year he launched Palantir\n",
      "Technologies, a software company that harnesses computers to empower human analysts in fields like\n",
      "\n",
      "but he could never create an entire industry. Startups operate on the principle that you need to work\n",
      "with other people to get stuff done, but you also need to stay small enough so that you actually can.\n",
      "Positively defined, a startup is the largest group of people you can convince of a plan to build a\n",
      "different future. A new company’s most important strength is new thinking: even more important than\n",
      "nimbleness, small size affords space to\n",
      "\n",
      "FOUNDATIONS\n",
      "E\n",
      "VERY GREAT COMPANY\n",
      " is unique, but there are a few things that every business must get right at the beginning.\n",
      "I stress this so often that friends have teasingly nicknamed it “Thiel’s law”: \n",
      "a startup messed up at\n",
      "its foundation cannot be fixed\n",
      ".\n",
      "Beginnings are special. They are qualitatively different from all that comes afterward. This was\n",
      "true 13.8 billion years ago, at the founding of our cosmos: in the earliest microseconds of its\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of the first document that was returned\n",
    "for doc in docs:\n",
    "    print (f\"{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d8504",
   "metadata": {},
   "source": [
    "### Option #2: Pinecone (for cloud)\n",
    "If you want to use pinecone, run the code below, if not then skip over to Chroma below it. You must go to [Pinecone.io](https://www.pinecone.io/) and set up an account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e093ef3",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', 'YourAPIKey')\n",
    "# PINECONE_API_ENV = os.getenv('PINECONE_API_ENV', 'us-east1-gcp') # You may need to switch with your env\n",
    "\n",
    "# # initialize pinecone\n",
    "# pinecone.init(\n",
    "#     api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "#     environment=PINECONE_API_ENV  # next to api key in console\n",
    "# )\n",
    "# index_name = \"langchaintest\" # put in the name of your pinecone index here\n",
    "\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "### Query those docs to get your answer back\n",
    "\n",
    "Great, those are just the docs which should hold our answer. Now we can pass those to a LangChain chain to query the LLM.\n",
    "\n",
    "We could do this manually, but a chain is a convenient helper for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f051337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "    temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9b1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_68036\\4041488970.py:1: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67ea7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'total_pages': 160, 'title': 'Zero to One: Notes on Startups, or How to Build the Future', 'page_label': '8', 'page': 7, 'creator': 'calibre 0.9.22 [http://calibre-ebook.com]', 'source': 'Zero to one.pdf', 'author': 'Peter Thiel & Blake Masters', 'producer': 'calibre 0.9.22 [http://calibre-ebook.com]', 'creationdate': '2014-09-30T04:23:17+00:00'}, page_content='THE CHALLENGE OF THE FUTURE\\nW\\nHENEVER\\n I \\nINTERVIEW\\n someone for a job, I like to ask this question: “What important truth do very few\\npeople agree with you on?”\\nThis question sounds easy because it’s straightforward. Actually, it’s very hard to answer. It’s\\nintellectually difficult because the knowledge that everyone is taught in school is by definition agreed\\nupon. And it’s psychologically difficult because anyone trying to answer must say something she'),\n",
       " Document(metadata={'source': 'Zero to one.pdf', 'creator': 'calibre 0.9.22 [http://calibre-ebook.com]', 'page_label': '144', 'author': 'Peter Thiel & Blake Masters', 'total_pages': 160, 'creationdate': '2014-09-30T04:23:17+00:00', 'page': 143, 'producer': 'calibre 0.9.22 [http://calibre-ebook.com]', 'title': 'Zero to One: Notes on Startups, or How to Build the Future'}, page_content='seize the unique opportunities we have to do new things in our own working lives. Everything\\nimportant to us—the universe, the planet, the country, your company, your life, and this very moment\\n—is singular.\\nOur task today is to find singular ways to create the new things that will make the future not just\\ndifferent, but better—to go from 0 to 1. The essential first step is to think for yourself. Only by seeing'),\n",
       " Document(metadata={'page_label': '8', 'author': 'Peter Thiel & Blake Masters', 'page': 7, 'source': 'Zero to one.pdf', 'title': 'Zero to One: Notes on Startups, or How to Build the Future', 'producer': 'calibre 0.9.22 [http://calibre-ebook.com]', 'total_pages': 160, 'creationdate': '2014-09-30T04:23:17+00:00', 'creator': 'calibre 0.9.22 [http://calibre-ebook.com]'}, page_content='the following form: “Most people believe in \\nx,\\n but the truth is the opposite of \\nx\\n.” I’ll give my own\\nanswer later in this chapter.\\nWhat does this contrarian question have to do with the future? In the most minimal sense, the future\\nis simply the set of all moments yet to come. But what makes the future distinctive and important isn’t\\nthat it hasn’t happened yet, but rather that it will be a time when the world looks different from today.'),\n",
       " Document(metadata={'title': 'Zero to One: Notes on Startups, or How to Build the Future', 'total_pages': 160, 'author': 'Peter Thiel & Blake Masters', 'creator': 'calibre 0.9.22 [http://calibre-ebook.com]', 'page_label': '59', 'creationdate': '2014-09-30T04:23:17+00:00', 'page': 58, 'source': 'Zero to one.pdf', 'producer': 'calibre 0.9.22 [http://calibre-ebook.com]'}, page_content='definite plan will always be underrated in a world where people see the future as random.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What he write in the challenge of the future ?\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e0a80",
   "metadata": {},
   "source": [
    "Awesome! We just went and queried an external data source!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3d1062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In \"The Challenge of the Future,\" the author writes about:\\n\\n*   **A unique interview question:** \"What important truth do very few people agree with you on?\" He notes this question is intellectually and psychologically difficult.\\n*   **The nature of the future:** He defines the future not just as moments yet to come, but as a time when the world will look *different* from today.\\n*   **Creating new things:** He emphasizes the task of finding singular ways to create new things (\"to go from 0 to 1\") that will make the future better.\\n*   **The importance of independent thinking:** He states that the essential first step is to \"think for yourself.\"\\n*   **The value of definite plans:** He suggests that a definite plan is often underrated in a world that perceives the future as random.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3c060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UV Python 3.11 (langchain-tutorial)",
   "language": "python",
   "name": "langchain-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
