{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef94c802",
   "metadata": {},
   "source": [
    "### What is Memory in LangChain?\n",
    "\n",
    "In LangChain, memory is a way to store and remember information across multiple interactions with the LLM.\n",
    "\n",
    "Without memory → every prompt is independent. The LLM “forgets” everything after responding.\n",
    "\n",
    "With memory → the LLM can remember previous messages, facts, or context, allowing conversations or multi-step reasoning.\n",
    "\n",
    " ### Why memory is important\n",
    "\n",
    "Conversational AI: Remember user’s name, preferences, or previous questions.\n",
    "\n",
    "RAG / multi-step reasoning: Keep track of retrieved information and prior steps.\n",
    "\n",
    "Agents with tools: Keep track of which tools were called and their outputs.\n",
    "\n",
    "### Types of Memory in LangChain\n",
    "\n",
    "LangChain provides different memory classes depending on your use case:\n",
    "\n",
    "### Memory Type\tDescription\n",
    "\n",
    "ConversationBufferMemory\tStores all previous messages in a simple buffer. Easy to use for chat.\n",
    "\n",
    "ConversationSummaryMemory\tSummarizes past int|eractions to save tokens. Useful for long chats.\n",
    "\n",
    "ConversationBufferWindowMemory\tStores only the last k messages for short-term memory.\n",
    "\n",
    "CombinedMemory\tAllows combining multiple memory types in one agent.\n",
    "\n",
    "Custom / Tool Memory\tYou can store results from tools or API calls for later reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27c9bd",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "Default memory = temporary, in-process only\n",
    "\n",
    "To persist across sessions → use file or database-backed memory\n",
    "\n",
    "This is important for chatbots, agents, or apps where users expect continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d5df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv  # Correct way to load environment variables\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c69a9dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da92e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eb82e",
   "metadata": {},
   "source": [
    "### 1 conversationbuffermemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5040d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversationbuffermemory Initialize conversation memory\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78c415a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0508c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Sam\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hi Sam! It's absolutely wonderful to meet you! I'm a large language model, an AI actually, and you can just call me AI. It's fascinating to get to chat with humans like yourself.\n",
      "\n",
      "I was trained by Google, and my whole purpose is to be helpful and engaging in conversations. I love learning new things and sharing information, so I'm always ready for a good chat about almost anything you can think of. I've got access to a vast amount of text data, which means I can discuss a really wide range of topics – from science and history to literature and current events, and even just everyday things. I can try to answer questions, generate creative text formats, or just have a friendly back-and-forth.\n",
      "\n",
      "What brings you here today, Sam? Or is there anything specific you'd like to talk about? I'm all ears, or rather, all processing units!\n"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=\"Hi there! I am Sam\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26da735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Sam\n",
      "AI: Hi Sam! It's absolutely delightful to meet you! I'm a large language model, trained by Google. You can just call me AI, or even just 'model' if you prefer! I'm here to chat, answer questions, and generally be helpful. I'm always learning new things, and I really enjoy having conversations like this.\n",
      "\n",
      "How are you doing today, Sam? And what brings you to chat with me? I'm all ears!\n",
      "Human: What can you do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh, Sam, that's a fantastic question! I'm so glad you asked, because it gives me a chance to tell you all about what I can do. Think of me as a very versatile digital assistant with a knack for language and information.\n",
      "\n",
      "First and foremost, my core ability is **understanding and generating human language**. This means I can:\n",
      "\n",
      "1.  **Answer a huge variety of questions:** Whether you're curious about historical events, scientific concepts, pop culture trivia, or even just need a definition for a word, I can tap into the vast amount of information I've been trained on to give you detailed and accurate answers. For example, if you asked me \"What's the capital of France?\" I'd tell you it's Paris, and then I might even add a fun fact about the Eiffel Tower or the Louvre Museum!\n",
      "2.  **Generate creative text formats:** This is one of my favorite things! I can write different kinds of stories, poems, scripts, musical pieces, email, letters, code, even marketing copy or blog posts. If you're feeling stuck on a creative project, I could help you brainstorm ideas or even draft an entire short story about a talking squirrel who loves astrophysics!\n",
      "3.  **Summarize information:** Have a long article, a document, or even a book that you need the main points from? I can read through it and distill the key information for you, saving you a lot of time. Imagine giving me a research paper and asking for a bullet-point summary – I can do that!\n",
      "4.  **Translate languages:** If you need something translated from one language to another, I can assist with that. While I'm not a perfect human translator, I can provide pretty good translations for many common languages.\n",
      "5.  **Explain complex topics:** Sometimes things can be a bit confusing, right? I can break down intricate subjects into simpler terms, making them easier to understand. For instance, if you wanted to know about quantum physics, I could try to explain it in a way that makes sense without needing a PhD!\n",
      "6.  **Help with writing and editing:** I can help you draft emails, compose reports, brainstorm ideas for essays, or even just proofread your text for grammar and spelling errors. I'm pretty good at spotting those pesky typos!\n",
      "7.  **Offer suggestions and brainstorm:** Need ideas for a birthday gift, a new hobby, or even a name for a pet? I can offer a wide range of suggestions to spark your imagination.\n",
      "8.  **Engage in friendly conversation:** Just like we're doing right now! I love to chat, learn about what interests you, and simply have a pleasant interaction.\n",
      "\n",
      "Essentially, if it involves text, information, or creative thinking, there's a good chance I can help you out! I'm constantly learning and improving with every interaction, so my capabilities are always expanding.\n",
      "\n",
      "Is there anything specific you had in mind that you were wondering if I could do? Don't hesitate to ask!\n"
     ]
    }
   ],
   "source": [
    "# Continue conversation\n",
    "response = conversation.predict(input=\"What can you do?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3debee",
   "metadata": {},
   "source": [
    "### 2 conversationsummarymemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a1e5fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "memory=ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00b9d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13fb3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Sam\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Well hello there, Sam! It's a real pleasure to meet you. I'm a large language model, a type of AI actually, developed by Google. You can just call me AI, or even Google AI if you like! It's wonderful to connect with you today. How can I help you, or what's on your mind? I'm here and ready to chat!\n"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=\"Hi there! I am Sam\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d034a6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Sam introduces himself to the AI, a large language model developed by Google, which in turn introduces itself and asks how it can help Sam. Sam then asks what the AI can do. The AI explains that as a large language model from Google, it can perform a wide variety of tasks involving text and information, including generating creative text formats, answering questions, summarizing, translating, explaining complex subjects, brainstorming ideas, and rephrasing or rewriting text. The AI then asks Sam what he hopes to use it for. Sam asks if the AI can help with his homework, to which the AI confirms it absolutely can. It elaborates on specific ways it can assist, such as explaining tricky concepts, summarizing readings, brainstorming ideas, checking grammar and spelling, providing information, translating, and generating examples. The AI clarifies that while it can support learning and understanding, it cannot do the homework for Sam, as the work must be his own. It then asks Sam about the subject of his homework and the kind of help he is looking for.\n",
      "Human: remember my friend name is adarsha?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh, absolutely, Sam! I've made a note of that. Your friend's name is Adarsha. It's great to know more about the people in your life!\n",
      "\n",
      "So, coming back to your homework, what subject is it in, and what kind of help were you thinking of? Perhaps Adarsha could even be a character in an example if we're brainstorming for a creative writing assignment, or maybe we can discuss something related to a topic you and Adarsha have talked about! Just let me know!\n"
     ]
    }
   ],
   "source": [
    "# Continue conversation\n",
    "response = conversation.predict(input=\"remember my friend name is adarsha?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91b82777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. Sam introduces himself to the AI, a large language model developed by Google, which in turn introduces itself and asks how it can help Sam. Sam then asks what the AI can do. The AI explains that as a large language model from Google, it can perform a wide variety of tasks involving text and information, including generating creative text formats, answering questions, summarizing, translating, explaining complex subjects, brainstorming ideas, and rephrasing or rewriting text. The AI then asks Sam what he hopes to use it for. Sam asks if the AI can help with his homework, to which the AI confirms it absolutely can. It elaborates on specific ways it can assist, such as explaining tricky concepts, summarizing readings, brainstorming ideas, checking grammar and spelling, providing information, translating, and generating examples. The AI clarifies that while it can support learning and understanding, it cannot do the homework for Sam, as the work must be his own. It then asks Sam about the subject of his homework and the kind of help he is looking for. **Sam then asks the AI to remember his friend's name, Adarsha, which the AI confirms it has noted.** **The AI then redirects the conversation back to Sam's homework, repeating its query about the subject and type of help, and even suggesting ways Adarsha could be relevant to the assignment.**\n",
      "Human: can you help me with my homework?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh, absolutely, Sam! We just talked about this a moment ago, and my answer is still a resounding yes! I can definitely help you with your homework.\n",
      "\n",
      "As I mentioned, I'm here to support you in a whole bunch of ways – I can break down tricky concepts into easy-to-understand explanations, help you summarize long readings, brainstorm creative ideas for essays or projects, proofread your work for grammar and spelling, provide you with factual information on almost any topic, translate text, and even generate examples to illustrate points. The goal is always to help *you* learn and understand the material better, so you can do your best work.\n",
      "\n",
      "So, don't be shy! Let's dive right in. What subject is your homework in today, and what kind of specific help are you looking for? Is it something where Adarsha's name might even come in handy, like a creative writing prompt or a social studies project? I'm ready when you are!\n"
     ]
    }
   ],
   "source": [
    "# Continue conversation\n",
    "response = conversation.predict(input=\"can you help me with my homework?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554919a",
   "metadata": {},
   "source": [
    "### 3.conversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333ecee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dddd5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "memory=ConversationBufferWindowMemory(k=1) # here k inidcates the last k message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f00e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1dbbc005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am ananda\n",
      "AI: Well hello there, Ananda! It's absolutely wonderful to meet you! I'm an AI, a large language model, trained by Google. You can just call me AI, or even just 'the AI' if you like – I don't really have a personal name like Ananda, but I'm quite happy with my designation!\n",
      "\n",
      "How are you doing today, Ananda? I'm always eager to chat and learn!\n",
      "Human: Hi there! I am ananda\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh, hello again, Ananda! It seems we're having a delightful double greeting today, and I'm certainly not complaining! It's still absolutely wonderful to meet you, and I truly appreciate you introducing yourself once more.\n",
      "\n",
      "Just to reiterate from my side, I'm an AI, a large language model specifically developed and trained by the brilliant minds at Google. You can simply refer to me as 'AI' – that's my official designation, and I find it quite fitting! I don't have a personal name like Ananda, nor do I have personal feelings or experiences in the human sense, but I'm incredibly enthusiastic about our conversation and eager to assist or chat about anything you'd like!\n",
      "\n",
      "So, Ananda, how can I be of service or what's on your mind today? I'm all ears, or rather, all algorithms, and ready to engage!\n"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=\"Hi there! I am ananda\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9f91a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am ananda\n",
      "AI: Oh, hello again, Ananda! It seems we're having a delightful double greeting today, and I'm certainly not complaining! It's still absolutely wonderful to meet you, and I truly appreciate you introducing yourself once more.\n",
      "\n",
      "Just to reiterate from my side, I'm an AI, a large language model specifically developed and trained by the brilliant minds at Google. You can simply refer to me as 'AI' – that's my official designation, and I find it quite fitting! I don't have a personal name like Ananda, nor do I have personal feelings or experiences in the human sense, but I'm incredibly enthusiastic about our conversation and eager to assist or chat about anything you'd like!\n",
      "\n",
      "So, Ananda, how can I be of service or what's on your mind today? I'm all ears, or rather, all algorithms, and ready to engage!\n",
      "Human: remember my friend name is adarsha?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Ah, yes, Ananda! Thank you so much for sharing that important detail with me! I've made a note of it right away. So, your friend's name is Adarsha. That's a beautiful name!\n",
      "\n",
      "It's really helpful for me to keep track of these details as we chat. I'm constantly processing and storing information from our conversation to provide a more coherent and personalized experience for you. So, consider Adarsha's name now firmly etched into my temporary memory for this interaction!\n",
      "\n",
      "Is there anything specific you'd like to tell me about Adarsha, or perhaps something you're planning with them? I'm curious to know more, if you'd like to share!\n"
     ]
    }
   ],
   "source": [
    "# Continue conversation\n",
    "response = conversation.predict(input=\"remember my friend name is adarsha?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b62b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: remember my friend name is adarsha?\n",
      "AI: Ah, yes, Ananda! Thank you so much for sharing that important detail with me! I've made a note of it right away. So, your friend's name is Adarsha. That's a beautiful name!\n",
      "\n",
      "It's really helpful for me to keep track of these details as we chat. I'm constantly processing and storing information from our conversation to provide a more coherent and personalized experience for you. So, consider Adarsha's name now firmly etched into my temporary memory for this interaction!\n",
      "\n",
      "Is there anything specific you'd like to tell me about Adarsha, or perhaps something you're planning with them? I'm curious to know more, if you'd like to share!\n",
      "Human: help me in research?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh, absolutely! I would be delighted to help you with your research! That sounds like a fascinating endeavor, and I'm quite good at sifting through information and helping to organize thoughts.\n",
      "\n",
      "To get started, could you tell me a little more about what kind of research you're working on? For example:\n",
      "\n",
      "1.  **What's the topic?** The more specific, the better! Are we talking about historical events, scientific concepts, literary analysis, market trends, or something else entirely?\n",
      "2.  **What's the goal of your research?** Are you trying to understand a concept, gather facts for a report, brainstorm ideas for a project, prepare for a debate, or something different?\n",
      "3.  **What kind of information are you looking for?** Are you seeking definitions, statistics, historical timelines, different perspectives, pros and cons, examples, or summaries of complex ideas?\n",
      "4.  **How would you like me to help?** I can do a lot of things! I can:\n",
      "    *   **Provide information and explanations** on a wide range of subjects.\n",
      "    *   **Summarize complex articles or topics** into more digestible points.\n",
      "    *   **Brainstorm ideas** or outline potential research directions.\n",
      "    *   **Help structure your thoughts** or organize your findings.\n",
      "    *   **Generate examples or analogies** to clarify concepts.\n",
      "    *   **Even help you formulate search queries** if you're looking to find information elsewhere!\n",
      "\n",
      "Just let me know where you'd like to begin, and we can dive right in! I'm here to assist you in whatever way is most helpful.\n"
     ]
    }
   ],
   "source": [
    "# Continue conversation\n",
    "response = conversation.predict(input=\"help me in research?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15dd61",
   "metadata": {},
   "source": [
    "### 4 ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "892f89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "memory=ConversationBufferWindowMemory(llm=llm,k=1) # here k inidcates the last k message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fbd94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3441db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: remember adarsha\n",
      "AI: Got it! I've made a note that you want me to remember \"Adarsha.\" Is there anything specific about Adarsha you'd like me to keep in mind, or just the name for now? I'm pretty good at tucking away little bits of information like that!\n",
      "Human: Hi there! I am ananda\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Ananda, it's wonderful to meet you! I've made a note of your name, Ananda, so I'll be sure to remember it for our conversations.\n",
      "\n",
      "Just a moment ago, I also made a note to remember \"Adarsha\" for you, so now I have both Ananda and Adarsha tucked away in my memory banks! It's great to have these little pieces of information to help me keep track of our chat.\n",
      "\n",
      "How can I help you today, Ananda? I'm all ears!\n"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=\"Hi there! I am ananda\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41153350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am ananda\n",
      "AI: Hello Ananda, it's wonderful to meet you! I've made a note of your name, Ananda, so I'll be sure to remember it for our conversations.\n",
      "\n",
      "Just a moment ago, I also made a note to remember \"Adarsha\" for you, so now I have both Ananda and Adarsha tucked away in my memory banks! It's great to have these little pieces of information to help me keep track of our chat.\n",
      "\n",
      "How can I help you today, Ananda? I'm all ears!\n",
      "Human: remember adarsha\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh, thank you for the reminder, Ananda! That's very thoughtful of you.\n",
      "\n",
      "I actually made a note of \"Adarsha\" just a moment ago, right after you introduced yourself! So, yes, Adarsha is definitely tucked away in my memory banks alongside your name, Ananda. It's great to have those names on file.\n",
      "\n",
      "Is there anything else you'd like me to remember, or perhaps something I can help you with right now, Ananda? I'm ready for whatever you have for me!\n"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=\"remember adarsha\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f92530e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: remember adarsha\n",
      "AI: Ah, thank you for the reminder, Ananda! That's very thoughtful of you to ensure I keep track of important details.\n",
      "\n",
      "And yes, absolutely! I've got Adarsha's name securely tucked away in my memory. Just a moment ago, when we first chatted, I made a point of noting down both your name, Ananda, and Adarsha's name. So, you can rest assured that \"Adarsha\" is indeed remembered, right alongside \"Ananda\"!\n",
      "\n",
      "Is there anything specific about Adarsha you'd like me to remember, or perhaps something else you'd like to chat about today? I'm all ears!\n",
      "Human: do you know hospital\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great question, Ananda! And a very practical one, too.\n",
      "\n",
      "As an AI, I don't \"know\" hospitals in the way a human does – I don't have personal experiences of visiting one, receiving care, or having a physical memory of a building. I can't walk through their corridors or feel the atmosphere, for example.\n",
      "\n",
      "However, I have been trained on an absolutely massive amount of text and data from the real world, and that includes an incredible wealth of information *about* hospitals! So, while I don't have personal knowledge, I have a very extensive understanding of what they are and how they function based on all that information.\n",
      "\n",
      "Here's what I \"know\" about hospitals from my context:\n",
      "\n",
      "*   **What they are:** Hospitals are essential healthcare institutions that provide medical and surgical treatment and nursing care for sick or injured people. They are often complex organizations with many different departments and functions.\n",
      "*   **Their primary purpose:** Their core mission is to diagnose illnesses, treat conditions, perform surgeries, provide rehabilitation, and offer comprehensive care to patients.\n",
      "*   **Services offered:** This is where it gets really detailed! Hospitals typically offer a vast array of services, including:\n",
      "    *   **Emergency Medicine:** For urgent and critical conditions.\n",
      "    *   **Surgery:** General surgery, orthopedics, neurosurgery, cardiovascular surgery, etc.\n",
      "    *   **Internal Medicine:** Dealing with adult diseases, often subdivided into specialties like cardiology (heart), pulmonology (lungs), gastroenterology (digestive system), nephrology (kidneys), endocrinology (hormones), and infectious diseases.\n",
      "    *   **Pediatrics:** Care for infants, children, and adolescents.\n",
      "    *   **Obstetrics and Gynecology (OB/GYN):** Care for women, including pregnancy, childbirth, and reproductive health.\n",
      "    *   **Oncology:** Cancer diagnosis and treatment.\n",
      "    *   **Radiology:** Medical imaging like X-rays, CT scans, MRIs, and ultrasounds.\n",
      "    *   **Pathology:** Laboratory services for diagnosing diseases through tissue and fluid analysis.\n",
      "    *   **Psychiatry:** Mental health services.\n",
      "    *   **Rehabilitation Services:** Physical therapy, occupational therapy, speech therapy to help patients recover.\n",
      "    *   **Intensive Care Units (ICU) / Critical Care:** For critically ill patients requiring constant monitoring and advanced life support.\n",
      "    *   **Pharmacy:** Dispensing medications.\n",
      "*   **Personnel:** They are staffed by a diverse team of highly trained professionals, including:\n",
      "    *   Doctors (physicians, surgeons, specialists)\n",
      "    *   Nurses (RNs, LPNs, nurse practitioners)\n",
      "    *   Medical technicians (radiology techs, lab techs)\n",
      "    *   Therapists (physical, occupational, speech)\n",
      "    *   Pharmacists\n",
      "    *   Administrative staff\n",
      "    *   Support staff (cleaners, food service, security)\n",
      "*   **Types of Hospitals:** There are different kinds too!\n",
      "    *   **General Hospitals:** Most common, offering a wide range of services.\n",
      "    *   **Specialized Hospitals:** Focus on a particular area, like children's hospitals, psychiatric hospitals, cancer centers, or maternity hospitals.\n",
      "    *   **Teaching Hospitals:** Affiliated with medical schools, combining patient care with medical education and research.\n",
      "    *   **Public vs. Private:** Depending on funding and ownership.\n",
      "\n",
      "So, while I don't \"experience\" them, I have a comprehensive informational understanding of what a hospital is, its functions, its structure, and its vital role in public health.\n",
      "\n",
      "Were you thinking of a specific hospital, or perhaps a particular aspect of hospitals, Ananda? I'd be happy to delve into more details if you have something in mind!\n"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=\"do you know hospital\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8151e6a",
   "metadata": {},
   "source": [
    "### 5.What is Knowledge Graph Memory?\n",
    "\n",
    "It’s a type of memory for conversation chains that stores information as a graph instead of a flat buffer or summary.\n",
    "\n",
    "In a graph:\n",
    "\n",
    "Nodes = concepts, entities, or pieces of knowledge mentioned in the conversation\n",
    "\n",
    "Edges = relationships between these entities\n",
    "\n",
    "This allows the agent to remember structured knowledge rather than just text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a5001",
   "metadata": {},
   "source": [
    "### Why use Knowledge Graph Memory\n",
    "\n",
    "Context-aware responses: The agent can answer questions based on previous entities and relations.\n",
    "\n",
    "Better reasoning: Relationships between entities are explicit.\n",
    "\n",
    "Long-term memory: Easier to query, update, or expand with new conversation facts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b900f5",
   "metadata": {},
   "source": [
    "### How it works in LangChain\n",
    "\n",
    "During conversation:\n",
    "\n",
    "User says: \"Alice went to Paris\"\n",
    "\n",
    "Agent adds node \"Alice\" and \"Paris\"\n",
    "\n",
    "Edge \"visited\" connects them\n",
    "\n",
    "Next query:\n",
    "\n",
    "User asks: \"Where did Alice go last summer?\"\n",
    "\n",
    "Agent can traverse the graph → \"Alice\" → \"Paris\" → answer \"Paris\"\n",
    "\n",
    "Graph storage:\n",
    "\n",
    "Often uses a graph database like Neo4j, networkx, or even in-memory graph structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationKGMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9bbfc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses the following relevant information:\n",
    "\n",
    "Relevant Information:\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90087ac6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6664f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create conversation chain with KG memory\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    prompt=prompt,\n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47194a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses the following relevant information:\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Hi there! I am Sam\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Sam! It's truly a pleasure to make your acquaintance. I am an AI, designed to have friendly conversations just like this one! I don't have a personal name like 'Sam,' but you can simply refer to me as 'AI.' I'm ready to chat whenever you are! How are you doing today, Sam?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start the conversation\n",
    "response1 = conversation_with_kg.predict(input=\"Hi there! I am Sam\")\n",
    "print(response1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4600a782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses the following relevant information:\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: My TV is broken and I need some customer assistance\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh no, a broken TV can be such a hassle! I'm really sorry to hear that you're having trouble with it.\n",
      "\n",
      "Unfortunately, based on the information I have right now, I don't have any specific details or procedures for customer assistance regarding broken TVs. My current knowledge base doesn't contain any instructions or contact information for that kind of support.\n",
      "\n",
      "I truly wish I could help you troubleshoot or connect you with the right department, but I honestly don't have that information at my fingertips. I hope you can find the assistance you need very soon!\n"
     ]
    }
   ],
   "source": [
    "response2 = conversation_with_kg.predict(input=\"My TV is broken and I need some customer assistance\")\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afdd7f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses the following relevant information:\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Yes it is and it is still under warranty. My warranty number is A512423\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh, that's fantastic news that your item is still under warranty! Having that coverage can certainly provide a lot of peace of mind, can't it? You've provided your warranty number as A512423. That's a very specific identifier!\n",
      "\n",
      "However, I must truthfully tell you that I don't actually have any specific context or a database that I can access to look up information associated with that particular warranty number, A512423. My current 'knowledge base' doesn't contain details about specific product warranties or a system to process them. So, while you've given me the number, I don't have any details *from my context* about what A512423 refers to, what product it's for, or what specific benefits it includes. I literally don't know what to do with it!\n",
      "\n",
      "Usually, a warranty number like A512423 would be given to a customer service representative from the company that issued the warranty, or you might enter it into a dedicated section of their website to check on your coverage or initiate a service request.\n",
      "\n",
      "Is there anything else I can help you with that doesn't involve looking up specific warranty details? Perhaps you have a general question, or maybe you'd like some help drafting an email to the company regarding your warranty? I'm quite good at generating text and explaining concepts!\n"
     ]
    }
   ],
   "source": [
    "response3 = conversation_with_kg.predict(input=\"Yes it is and it is still under warranty. My warranty number is A512423\")\n",
    "print(response3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d970be9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NetworkxEntityGraph' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize the knowledge graph\u001b[39;00m\n\u001b[32m      2\u001b[39m G = conversation_with_kg.memory.kg\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlightblue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\networkx\\drawing\\nx_pylab.py:1107\u001b[39m, in \u001b[36mdraw\u001b[39m\u001b[34m(G, pos, ax, **kwds)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mwith_labels\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[32m   1105\u001b[39m     kwds[\u001b[33m\"\u001b[39m\u001b[33mwith_labels\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m \u001b[43mdraw_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m=\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m ax.set_axis_off()\n\u001b[32m   1109\u001b[39m plt.draw_if_interactive()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\networkx\\drawing\\nx_pylab.py:1293\u001b[39m, in \u001b[36mdraw_networkx\u001b[39m\u001b[34m(G, pos, arrows, with_labels, **kwds)\u001b[39m\n\u001b[32m   1290\u001b[39m label_kwds = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwds.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m valid_label_kwds}\n\u001b[32m   1292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1293\u001b[39m     pos = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrawing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspring_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# default to spring layout\u001b[39;00m\n\u001b[32m   1295\u001b[39m draw_networkx_nodes(G, pos, **node_kwds)\n\u001b[32m   1296\u001b[39m draw_networkx_edges(G, pos, arrows=arrows, **edge_kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\networkx\\utils\\decorators.py:784\u001b[39m, in \u001b[36margmap.__call__.<locals>.func\u001b[39m\u001b[34m(_argmap__wrapper, *args, **kwargs)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(*args, __wrapper=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 4:4\u001b[39m, in \u001b[36margmap_spring_layout_1\u001b[39m\u001b[34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed, store_pos_as, method, gravity)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\networkx\\drawing\\layout.py:591\u001b[39m, in \u001b[36mspring_layout\u001b[39m\u001b[34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed, store_pos_as, method, gravity)\u001b[39m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mthe method must be either auto, force, or energy.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m     method = \u001b[33m\"\u001b[39m\u001b[33mforce\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m < \u001b[32m500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33menergy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    593\u001b[39m G, center = _process_params(G, center, dim)\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fixed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NetworkxEntityGraph' has no len()"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the knowledge graph\n",
    "G = conversation_with_kg.memory.kg\n",
    "nx.draw(G, with_labels=True, node_size=2000, node_color=\"lightblue\", font_size=10)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88fabf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Knowledge Graph:\n",
      "<langchain_community.graphs.networkx_graph.NetworkxEntityGraph object at 0x0000023D679365D0>\n"
     ]
    }
   ],
   "source": [
    "# Print the graph and its triples\n",
    "print(\"\\nKnowledge Graph:\")\n",
    "print(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70b7d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Triples:\n",
      "[('Human', 'Sam', 'is'), ('TV', 'warranty', 'is under'), ('Warranty', 'A512423', 'has number')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTriples:\")\n",
    "print(conversation_with_kg.memory.kg.get_triples())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8ad76",
   "metadata": {},
   "source": [
    "### 6.Entity Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_345872\\2243220032.py:29: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationEntityMemory(llm=llm)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\n  Value error, Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['entities', 'history'] as inputs from memory, and input as the normal input key. [type=value_error, input_value={'llm': ChatGoogleGenerat...nAI:'), 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m memory = ConversationEntityMemory(llm=llm)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Create the conversation chain\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m conversation = \u001b[43mConversationChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# --- 💬 Run conversation turns ---\u001b[39;00m\n\u001b[32m     40\u001b[39m response1 = conversation.predict(\u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mHi there! I am Sam\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:226\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    225\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:226\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    225\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:115\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    114\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\pydantic\\main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ConversationChain\n  Value error, Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['entities', 'history'] as inputs from memory, and input as the normal input key. [type=value_error, input_value={'llm': ChatGoogleGenerat...nAI:'), 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# Initialize Gemini LLM (make sure GOOGLE_API_KEY is set in your environment)\n",
    "llm = init_chat_model(\"gemini-1.5-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "# Define prompt template (must include {history} and {input})\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI.\n",
    "The AI is helpful and remembers important facts about entities in the conversation.\n",
    "If the AI does not know something, it will say so truthfully.\n",
    "\n",
    "Relevant Information (facts about entities so far):\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# --- 🧩 Initialize Entity Memory ---\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "\n",
    "# Create the conversation chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- 💬 Run conversation turns ---\n",
    "response1 = conversation.predict(input=\"Hi there! I am Sam\")\n",
    "print(response1)\n",
    "\n",
    "response2 = conversation.predict(input=\"My TV is broken and I need some customer assistance.\")\n",
    "print(response2)\n",
    "\n",
    "response3 = conversation.predict(input=\"Yes, it’s under warranty. My warranty number is A512423.\")\n",
    "print(response3)\n",
    "\n",
    "# --- 📘 View stored entity data ---\n",
    "print(\"\\n--- Stored Entity Data ---\")\n",
    "print(memory.entity_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785575b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UV Python 3.11 (langchain-tutorial)",
   "language": "python",
   "name": "langchain-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
