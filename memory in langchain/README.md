# üß† Memory in LangChain

## üìã Overview

This folder contains implementations and examples of different memory types in LangChain. Memory allows LLMs to remember information across multiple interactions, enabling contextual conversations and multi-step reasoning.

## üéØ What is Memory in LangChain?

Memory in LangChain is a mechanism to store and retrieve information across multiple interactions with an LLM.

**Without Memory** ‚Üí Every prompt is independent. The LLM "forgets" everything after responding.

**With Memory** ‚Üí The LLM can remember previous messages, facts, or context, allowing for:
- Conversational AI with context awareness
- RAG systems with multi-step reasoning
- Agents that track tool usage and outputs
- Persistent user preferences and history

## üìÅ Files in this Folder

| File | Description |
|------|-------------|
| `memory.ipynb` | Comprehensive notebook demonstrating all memory types |

## üîß Memory Types Implemented

### 1Ô∏è‚É£ ConversationBufferMemory

**What it does:** Stores ALL previous messages in a simple buffer

**Best for:**
- Short conversations
- When you need complete conversation history
- Applications with sufficient token budget

**Key Features:**
- ‚úÖ Complete conversation context
- ‚úÖ Simple implementation
- ‚ùå Can consume many tokens in long conversations

```python
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory

memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)

response = conversation.predict(input="Hi there! I am Sam")
# LLM remembers: "User said their name is Sam"
```

---

### 2Ô∏è‚É£ ConversationSummaryMemory

**What it does:** Summarizes past interactions to save tokens

**Best for:**
- Long conversations
- Token-limited applications
- When you need context without full history

**Key Features:**
- ‚úÖ Token-efficient for long conversations
- ‚úÖ Maintains essential context
- ‚ùå May lose some details in summarization
- ‚ö†Ô∏è Requires additional LLM calls for summarization

```python
from langchain.chains.conversation.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(llm=llm)
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)

response = conversation.predict(input="Hi there! I am Sam")
# Creates summary: "User introduced themselves as Sam"
```

---

### 3Ô∏è‚É£ ConversationBufferWindowMemory

**What it does:** Stores only the last `k` messages for short-term memory

**Best for:**
- Recent context tracking
- Memory-constrained applications
- When only recent interactions matter

**Key Features:**
- ‚úÖ Fixed memory size
- ‚úÖ Predictable token usage
- ‚ùå Forgets older interactions
- ‚öôÔ∏è Configurable window size with `k` parameter

```python
from langchain.chains.conversation.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=2)  # Remember last 2 exchanges
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)

# After 3 exchanges, only the last 2 are retained
```

---

### 4Ô∏è‚É£ ConversationSummaryBufferMemory

**What it does:** Combines buffer and summary approaches - keeps recent messages in full and summarizes older ones

**Best for:**
- Balanced approach between context and efficiency
- Long conversations needing recent precision
- Production chatbots

**Key Features:**
- ‚úÖ Recent messages in full detail
- ‚úÖ Older messages summarized
- ‚úÖ Good balance of context vs. tokens
- ‚öôÔ∏è Configurable with `max_token_limit`

```python
from langchain.chains.conversation.memory import ConversationSummaryBufferMemory

memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)
```

---

### 5Ô∏è‚É£ ConversationKGMemory (Knowledge Graph Memory)

**What it does:** Stores information as a knowledge graph with entities and relationships

**Best for:**
- Complex entity tracking
- Relationship-based reasoning
- Domain-specific knowledge extraction
- Multi-entity conversations

**Key Features:**
- ‚úÖ Structured knowledge representation
- ‚úÖ Entity relationship mapping
- ‚úÖ Efficient querying of facts
- ‚úÖ Better reasoning capabilities
- üîó Can integrate with graph databases (Neo4j, NetworkX)

**How it works:**
1. **Entity Extraction:** Identifies entities (people, places, concepts) from conversation
2. **Relationship Mapping:** Connects entities with relationships
3. **Graph Traversal:** Queries the graph to answer questions

**Example:**
```
User: "Alice went to Paris and met Bob there"
Graph Structure:
- Node: Alice (person)
- Node: Paris (location)
- Node: Bob (person)
- Edge: Alice --[visited]--> Paris
- Edge: Alice --[met]--> Bob
- Edge: Bob --[located_in]--> Paris

Next Query: "Where did Alice go?"
Answer: "Paris" (by traversing Alice ‚Üí visited ‚Üí Paris)
```

```python
from langchain.chains.conversation.memory import ConversationKGMemory

# Initialize Knowledge Graph Memory
kg_memory = ConversationKGMemory(llm=llm)
conversation = ConversationChain(llm=llm, memory=kg_memory, verbose=True)

# Build knowledge graph through conversation
conversation.predict(input="Alice works at Google")
conversation.predict(input="Bob is Alice's manager")
conversation.predict(input="Google is located in Mountain View")

# Query relationships
conversation.predict(input="Who is Alice's manager?")
# LLM can traverse: Alice ‚Üí managed_by ‚Üí Bob
```

---

## üìä Memory Comparison Table

| Memory Type | Token Usage | Context Retention | Best Use Case | Complexity |
|-------------|-------------|-------------------|---------------|------------|
| **ConversationBufferMemory** | High (all messages) | Complete | Short chats | Low |
| **ConversationSummaryMemory** | Low (summarized) | High-level only | Long conversations | Medium |
| **ConversationBufferWindowMemory** | Fixed (k messages) | Recent only | Recent context | Low |
| **ConversationSummaryBufferMemory** | Medium (balanced) | Recent + summary | Production apps | Medium |
| **ConversationKGMemory** | Low (entities only) | Structured facts | Entity tracking | High |

## üîë Key Concepts

### Memory Persistence
- **Default:** Memory is temporary and in-process only
- **For Production:** Use file or database-backed memory for persistence across sessions
- **Important for:** Chatbots, agents, or apps requiring continuity

### When to Use Which Memory?

```
üìù Short conversations + full context needed
   ‚Üí ConversationBufferMemory

üìö Long conversations + token optimization
   ‚Üí ConversationSummaryMemory

‚è∞ Only recent context matters
   ‚Üí ConversationBufferWindowMemory

‚öñÔ∏è Balance between recency and history
   ‚Üí ConversationSummaryBufferMemory

üï∏Ô∏è Complex entity relationships + structured knowledge
   ‚Üí ConversationKGMemory (Knowledge Graph)
```

## üí° Use Cases

### 1. Customer Support Chatbot
```python
# Remember customer name, issue, and previous interactions
memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=200)
```

### 2. Research Assistant
```python
# Track entities, papers, authors, and their relationships
memory = ConversationKGMemory(llm=llm)
```

### 3. Quick Q&A Bot
```python
# Only need last few exchanges
memory = ConversationBufferWindowMemory(k=3)
```

### 4. Personal AI Assistant
```python
# Remember everything about the user
memory = ConversationBufferMemory()
```

## üöÄ Getting Started

### Prerequisites
```bash
pip install langchain langchain-google-genai python-dotenv
```

### Basic Setup
```python
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain.chat_models import init_chat_model
from dotenv import load_dotenv

# Load API keys
load_dotenv()

# Initialize LLM
llm = init_chat_model("gemini-2.5-flash", model_provider="google_genai")

# Create memory
memory = ConversationBufferMemory()

# Create conversation chain
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# Start chatting!
response = conversation.predict(input="Hi! I'm learning about LangChain memory")
```

## üîç Advanced Features

### Combining Multiple Memory Types
```python
from langchain.memory import CombinedMemory

# Use multiple memory strategies together
combined_memory = CombinedMemory(memories=[
    ConversationBufferWindowMemory(k=3),
    ConversationKGMemory(llm=llm)
])
```

### Custom Memory Storage
```python
# Store memory in external database
# Useful for persistent conversations across sessions
```

### Memory with Tools/Agents
```python
# Track tool calls and their results
# Remember which APIs were called and their outputs
```

## üìñ Learning Path

1. **Start with:** `ConversationBufferMemory` (simplest)
2. **Experiment with:** `ConversationBufferWindowMemory` (understand windowing)
3. **Optimize with:** `ConversationSummaryMemory` (token efficiency)
4. **Balance with:** `ConversationSummaryBufferMemory` (production-ready)
5. **Advanced:** `ConversationKGMemory` (structured knowledge)

## üéì Key Takeaways

‚úÖ Memory enables contextual conversations and multi-step reasoning

‚úÖ Different memory types trade-off between context retention and token usage

‚úÖ ConversationKGMemory provides structured knowledge representation

‚úÖ Production apps should use persistent storage for memory

‚úÖ Choose memory type based on conversation length and use case

## üîó Related Concepts

- **Chains:** Memory is often used with ConversationChain
- **Agents:** Agents use memory to track tool usage
- **RAG Systems:** Memory helps maintain context across document retrievals
- **Prompt Templates:** Memory injects conversation history into prompts

## üìö Resources

- [LangChain Memory Documentation](https://python.langchain.com/docs/modules/memory/)
- [Knowledge Graphs in AI](https://neo4j.com/developer/knowledge-graph/)
- [Conversation Design Best Practices](https://developers.google.com/assistant/conversational/design)

---

**Created for LangChain Tutorial Repository**  
Demonstrates practical implementations of conversation memory in LangChain applications.
